{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring nonparametric methods - Boosted Trees and ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lecture, two new methods were introduced: (Gradient) Boosting on the example of Decision Trees and Artificial Neural Networks (ANNs). These two methods are quite different. Boosting is an ensemble technique that in principal can be applied to any statistical learning method (but is mostly used with Decision Trees) to improve an otherwise unsatisfying result. ANNs, on the other hand, are a completely new, and slightly special statistical learning method that we will explore a bit in this tutorial.\n",
    "\n",
    "In the following, we will first illustrate how the boosting algorithm works on our already well-known 1D artificial dataset. Then, we will apply it to our 2D artificial dataset and see how Cross-validation helps finding optimal parameters. Then, we will move on to the ANNs. We will fit a simple ANN to our 2D artificial dataset and explore Cross-Validation. \n",
    "\n",
    "Finally, both methods are applied to our PSL/precipitation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(raster)\n",
    "library(rworldmap)\n",
    "library(RColorBrewer)\n",
    "library(gbm)\n",
    "library(rpart)\n",
    "library(nnet)\n",
    "\n",
    "source('functions.R')\n",
    "source('../Exercise2/functions.R')\n",
    "source('../Exercise3/functions.R')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting - Example of Boosted Trees on 1D artificial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You surely remember our 1D polynomial artificial data, first introduced in the first Exercise. We load it and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load function to create dataset from exercise 1:\n",
    "source('../Exercise1/functions.R')\n",
    "\n",
    "## Generate 200 training sample points:\n",
    "train.data = generate.data.poly(N=200, seed=200, sd.noise=0.15)\n",
    "\n",
    "## Plot true function and training dataset:\n",
    "plot(train.data$X, train.data$Y_true, lwd=2, type=\"l\", ylim=c(-0.7,1),axes=F, xaxt='n', ann=FALSE)\n",
    "points(train.data$X, train.data$Y,col=\"darkgrey\")\n",
    "legend(\"topleft\",legend=c(\"True\",\"Train\"),col=c(\"black\",\"darkgrey\"),\n",
    "       lty=c(1,NA),pch=c(NA,1),lwd=2, bty = \"n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up a Boosted Trees model. Initially, we constrain each tree to only split up into two branches by setting `maxdepth_tree` to `1`. This means that each single Tree is only able to predict one of two different values out of any possible inputs given. Obviously, this is not a very good setting for predicting a polynomial function. But it helps to illustrate how Boosting is able to improve the prediction of weak learners.\n",
    "\n",
    "We train the first tree and plot its prediction together with the training data. The red lines mark the residuals, i.e. the difference between the model prediction and the data points from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the maximum depth of each single tree:\n",
    "maxdepth_tree <- 1\n",
    "\n",
    "## Traing first tree on the training dataset:\n",
    "train.data.model <- train.data\n",
    "xy.stump.1 <- rpart(Y~X,data=train.data,control=rpart.control(maxdepth=maxdepth_tree))\n",
    "\n",
    "## Add residuals and predictions (at training datapoints) to dataframe:\n",
    "train.data.model[\"resid_stump_1\"] <- resid(xy.stump.1)\n",
    "train.data.model[\"pred_stump_1\"]  <- predict(xy.stump.1, newdata=data.frame(X=train.data.model$X))\n",
    "                      \n",
    "## Make prediction for complete x-axis (not just training datapoints):\n",
    "x_pred_all <- seq(0,15,0.01)\n",
    "y_pred_all <- data.frame(predict(xy.stump.1, newdata=data.frame(X=x_pred_all)))\n",
    "                        \n",
    "## Plot prediction of first tree and residuals:\n",
    "plot(train.data.model$X, train.data$Y, ylim=c(-0.7,1),axes=F, xaxt='n', ann=FALSE, col=\"darkgrey\")\n",
    "segments(x0=train.data.model$X, x1=train.data.model$X,\n",
    "         y0=train.data.model$pred_stump_1, y1=train.data.model$Y, col=\"pink\")\n",
    "lines(x_pred_all, y_pred_all[,1], col=\"red\",lwd=2)\n",
    "legend(\"topleft\", legend=c(\"Prediction of Tree 1\", \"Residuals\"), lwd=c(2,1), col=c(\"red\",\"pink\"), bty = \"n\")\n",
    "title(main=\"Tree 1\",xlab=\"x\",ylab=\"y\")\n",
    "abline(h=0)\n",
    "axis(2)\n",
    "\n",
    "## Get settings for the next plots:\n",
    "range_resid_1 <- range(train.data.model[\"pred_stump_1\"]-train.data$Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the iterative part of the Boosting begins. The next tree is not fitted on the original training data, but on the *residuals* that have been produced by the first tree. This happens iteratively for each next tree until the maximum allowed number of trees (`n_stumps <- 10`) is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define number of trees to be fitted successifely to residuals:\n",
    "n_stumps      <- 10\n",
    "\n",
    "## Define number of trees to be fitted successifely to residuals:\n",
    "plot_n_stumps <- 4\n",
    "\n",
    "## This can be ignored (just defined such that cell works if executed repeatedly)\n",
    "y_pred_all <- data.frame(predict(xy.stump.1, newdata=data.frame(X=x_pred_all)))\n",
    "train.data.model[\"pred_stump_cumsum_1\"] <- train.data.model$pred_stump_1\n",
    "\n",
    "par(mfrow=c(plot_n_stumps-1,2))\n",
    "for(n in 2:n_stumps) {\n",
    "  ## Make prediction on residuals of last tree:\n",
    "  formula    <- as.formula(paste0(\"resid_stump_\",n-1,\" ~ X\"))\n",
    "  xy.stump.n <- rpart(formula,data=train.data.model,control=rpart.control(maxdepth=maxdepth_tree))\n",
    "    \n",
    "  ## Add residuals and predictions (at training datapoints) to dataframe:\n",
    "  train.data.model[paste0(\"resid_stump_\",n)] <- resid(xy.stump.n)\n",
    "  train.data.model[paste0( \"pred_stump_\",n)] <- predict(xy.stump.n, newdata=data.frame(X=train.data.model$X))\n",
    "  train.data.model[paste0( \"pred_stump_cumsum_\",n)] <- train.data.model[paste0(\"pred_stump_cumsum_\",n-1)] +\n",
    "                                                       train.data.model[paste0(\"pred_stump_\",n)]\n",
    " \n",
    "  ## Make prediction for complete x-axis (not just training datapoints):\n",
    "  y_pred_all_n <- predict(xy.stump.n, newdata=data.frame(X=x_pred_all))\n",
    "  y_pred_all   <- cbind(y_pred_all, y_pred_all[,(n-1)] + y_pred_all_n); names(y_pred_all) <- 1:n\n",
    "    \n",
    "  if(n <= plot_n_stumps) {\n",
    "    ## Plot residuals of combined tree prediction (so far) and prediction with additional tree:\n",
    "    plot(train.data.model$X, train.data.model[,paste0(\"resid_stump_\",n-1)], type=\"h\",\n",
    "         ylim=range_resid_1, axes=F, xaxt='n', ann=FALSE, col=\"pink\")\n",
    "    abline(h=0)\n",
    "    lines(x_pred_all, y_pred_all_n,col=\"red\",lwd=2)\n",
    "    title(main=paste0(\"Residuals of former fit and tree \",n))\n",
    "    \n",
    "    ## Plot training data and combined tree prediction and new residuals:\n",
    "    #y_pred_all <- y_pred_all + y_pred_all_n\n",
    "    plot(train.data.model$X, train.data.model$Y, ylim=c(-0.7,1),axes=F, xaxt='n', ann=FALSE, col=\"darkgrey\")\n",
    "    segments(x0=train.data.model$X, x1=train.data.model$X,\n",
    "             y0=train.data.model[,paste0( \"pred_stump_cumsum_\",n)], y1=train.data.model$Y, col=\"pink\")\n",
    "    lines(x_pred_all,y_pred_all[,n],col=\"red\",lwd=2)\n",
    "    title(main=paste0(\"Trees 1-\",n))\n",
    "    abline(h=0)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see how the prediction gets more accurate for each added tree. We can also plot the change of the prediction with each added tree as an evolution, where the final prediction after iteratively fitting 10 trees on the residuals of the tree before is shown with the yellow line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot evolution of all successive tree changes:\n",
    "line_colors <- hcl.colors(n_stumps, palette = \"viridis\")\n",
    "plot(train.data$X, train.data$Y_true, lwd=2, axes=F, xaxt='n', ann=FALSE, ylim=c(-0.7,1), type=\"l\")\n",
    "points(train.data.model$X, train.data.model$Y,col=\"darkgrey\")\n",
    "matlines(x_pred_all, y_pred_all, col=line_colors,lty=1,lwd=2)\n",
    "legend(\"topleft\", legend=c(\"Truth\", paste0(\"Prediction of Trees 1-\",1:n_stumps)), lwd=1,\n",
    "       col=c(\"black\", line_colors), bty = \"n\", cex=0.6, ncol=3)\n",
    "abline(h=0); axis(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example also illustrates why trees are not quite ideal to fit continuous functions, and often are not ideal if problems are \"too linear\" (i.e., even with a lot of flexibility the tree can only predict something discountinous...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions</b> \n",
    "\n",
    "- Are you happy with the boosting result?\n",
    "- Can you explain, why the boosting has troubles fitting this data?\n",
    "- Based on your explanation, what would be a strategy to improve to model?\n",
    "- Set ```maxdepth_tree <- 2``` (top of third cell above) to see whether an improvement is possible.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting - Example of Boosted Trees on 2D artificial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the same non-linear 2D artificial dataset as for the last exercise and split it into 50% training and 50% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create explanatory variables\n",
    "yy.lim <- xx.lim <- c(-10,10)\n",
    "xx     <- seq(min(xx.lim),max(xx.lim),length.out=100)\n",
    "yy     <- seq(min(yy.lim),max(yy.lim),length.out=100)\n",
    "xy.mat <- expand.grid(x=xx,y=yy)\n",
    "\n",
    "## Function for creating artificial response variable\n",
    "zz.fun <- function(x,y){\n",
    "    r <- sqrt(x^2+y^2)\n",
    "    ro <- 10 * sin(r)/r\n",
    "    return(ro)\n",
    "}\n",
    "\n",
    "## Compute z for all combinations x and y\n",
    "data   <- data.frame(xy.mat,z=zz.fun(x=xy.mat$x,y=xy.mat$y))\n",
    "N.data <- nrow(data)\n",
    "\n",
    "## Add Gaussian noise (sd=0.5) to z-values:\n",
    "set.seed(101)   \n",
    "noise.sd <- 0.5\n",
    "data$z.noisy <- data$z + rnorm(nrow(data),mean=0,sd=noise.sd)\n",
    "\n",
    "## Let's use half of the data for training:\n",
    "frac.test <- 0.5\n",
    "\n",
    "## Select training and testdata:\n",
    "test.ind   <- sample(N.data,round(frac.test*N.data))\n",
    "test.data  <- data[test.ind,]\n",
    "train.data <- data[-test.ind,]\n",
    "\n",
    "## Sizes of training and test data\n",
    "N.train.data <- nrow(train.data)\n",
    "N.test.data  <- nrow(test.data)\n",
    "\n",
    "## Plot true function:\n",
    "par(mfrow=c(1,1))\n",
    "plot.xyz.persp(x=data$x,y=data$y,z=data$z,axnames=T,title=\"True function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try the Boosted Tree method on this dataset, albeit with a low number of trees (5) and, as above, each tree is only allowed to split once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Decide on function settings:\n",
    "distribution_set  <- \"gaussian\" ## For squared error loss (for regression problem)\n",
    "n_trees           <- 5          ## Number of trees\n",
    "interaction_depth <- 1          ## Tree size (number of splits)\n",
    "shrinkage_par     <- 1          ## \"Regularisation\"\n",
    "\n",
    "## Fit boosted trees to droplet dataset:\n",
    "gbm.fitted <- gbm(z.noisy~x+y, data=train.data,\n",
    "                  distribution=distribution_set,\n",
    "                  n.tree=n_trees,\n",
    "                  interaction.depth=interaction_depth,\n",
    "                  shrinkage=shrinkage_par,\n",
    "                  bag.fraction=1)\n",
    "\n",
    "## Plot prediction:\n",
    "plot.xyz.persp(x=data$x,y=data$y,z=predict(gbm.fitted,newdata=data,n.tree=n_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions</b> \n",
    "\n",
    "- What could be an explanation for the cross-like shape of the prediction?\n",
    "- Does increasing the number of trees help (e.g. setting ```n_trees <- 50```)?\n",
    "- Leaving ```n_trees <- 50```, what is the effect of decreasing the regularisation ```shrinkage_par <- 0.1``` to a lower value (such that it actually has an influence)?\n",
    "- Now try roughly optimal parameters by your own, what do you end up with?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation on Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen that Boosted Trees improve results, but the values we used for the number of trees `n_trees`and the number of allowed branches or splits per tree `interaction_depth` were probably too low to fit a model complex enough to follow the non-linear training data.\n",
    "\n",
    "But, luckily, we know a procedure that helps us determine useful values for these hyperparameters - mighty Cross-Validation! Exemplarily, we try to find the optimal number of trees with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find optimal number of trees with the following function:\n",
    "gbm.fitted <- gbm(z.noisy~x+y, data=train.data, distribution=distribution_set,\n",
    "                  n.tree=n_trees, interaction.depth=interaction_depth,\n",
    "                  shrinkage=shrinkage_par, bag.fraction=1, cv=5)\n",
    "best.n.tree <- gbm.perf(gbm.fitted,method=\"cv\")\n",
    "sprintf(\"Optimal number of trees: %i\", best.n.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions</b> \n",
    "\n",
    "- Have a look at the help page of ```gbm.perf```-function and try to understand what it does.\n",
    "- What do the curves in the plot show?\n",
    "- Did you fit enough trees?\n",
    "- Try to reproduce the training/validation-error plot on slide 14 (```shrinkage_par <- 1```)\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks - on 2D artificial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks consist of an **input layer**, a variable number of **hidden layers** and an **output layer**. Each of these layers consist of a number of **nodes**. The input layer takes the table of predictands and therefore has as many nodes as there are predictands. The output layer has as many nodes as there are predictors, usually one. The hidden layers can in principle have any number of hidden units. More than one hidden layer is usually refered to as Deep Learning. All of the units of each layer are connected with all units from the previous and the next layer. \n",
    "\n",
    "Once a datapoint is *fed* into the Neural Net for prediction, it is passed on through all the nodes until it arrives at the output node, which produces a value for the prediction. Each node, being confronted with a value, applies the globally defined **activation function** on the value, adds the individual **bias** term and passes it on through its weight connection th all the nodes in the next layer, albeit again exposed to another weight multiplication.\n",
    "\n",
    "This architecture is mathematically quite convoluted to describe but quite intuitive to understand once visualised as a net of neurons, propagating information towards the output node:\n",
    "\n",
    "<img src=\"figures/ann_arch.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a small ANN to the artificial 2D data. Neural Nets tend to have a lot of hyperparameters. We only use the two most important ones explained below:\n",
    "\n",
    "- `maxit` is the maximum number of iterations the ANN will go through the training data to adjust its weights.\n",
    "- `size` is the number of hidden units. Per default, the `nnet` package only allows singular hidden units. The number of units of the input layer is the number of variables from `train.data` (here, `x` and `y`) and the number of output units is the number of predictors (here, `z.noisy`). In principle, ANNs can have an unlimited amount of hidden layers (called deep learning) and several output layers. We limit ourselves to a small ANN for this tutorial.\n",
    "\n",
    "The fitting procedure can be observed by the printout of the following piece of code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define maximum number of iterations for fitting the ANN\n",
    "nn.maxit <- 500\n",
    "\n",
    "## Fit network of size=5:\n",
    "set.seed(456)\n",
    "nn.fit.size5 <- nnet(z.noisy~x+y, data=train.data,\n",
    "                     size=5,\n",
    "                     maxit=nn.maxit,linout=TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANN has converged, that means the fitting process is finished. We can inspect the weights of each node that have been learned (= adjusted) during the fitting procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print fitted weigths:\n",
    "sprintf(\"Number of weights: %i\", length(nn.fit.size5$wts))\n",
    "print(nn.fit.size5$wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions</b> \n",
    "\n",
    "- Try to draw a figure of the network structure.\n",
    "\n",
    "- From the output above, ...<br>... is there evidence that the model overfits the data?<br>... can you explain the number of fitted weights?\n",
    "\n",
    "    \n",
    "- It might helps to check out ```summary(nn.fit.size5)``` :)\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANNs are best understood when looking at a visual sketch of it. Remember we have 2 inputs (`x`and `y`) and one output (`z`) and set `size` to 5, resulting in 21 weights. The following sketch shows the resulting ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot neural net:\n",
    "plot_ann(n_input=2,n_hidden=5,weights=nn.fit.size5$wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a fitted ANN, we can use it for predicting on the test dataset. For comparison, we also fit and predict with a large ANN (`size <- 50`) to see the impact of increasing the number of hidden units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make prediction with small ANN on test-set (for MSE) and full dataset (for plotting):\n",
    "nn.test.size5.pred <- predict(nn.fit.size5,newdata=test.data)\n",
    "nn.pred.size.5 <- predict(nn.fit.size5,newdata=data)\n",
    "\n",
    "## Fit a more complex model (hidden layer with 50 nodes):\n",
    "nn.fit.size50 <- nnet(z.noisy~x+y,data=train.data,size=50,\n",
    "                      maxit=nn.maxit,linout=TRUE, trace=F)\n",
    "\n",
    "## Make prediction on test-set (for MSE) and full dataset (for plotting):\n",
    "nn.test.size50.pred <- predict(nn.fit.size50,newdata=test.data)\n",
    "nn.pred.size.50 <- predict(nn.fit.size50,newdata=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predictive power of both ANNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(1,2))\n",
    "plot.xyz.persp(x=data$x,y=data$y,z=nn.pred.size.5)\n",
    "legend(\"topright\",legend=c(sprintf(\"MSE: %.2f\",MSE(obs=test.data$z.noisy,pred=nn.test.size5.pred)),\n",
    "                           sprintf(\"R2: %.2f\",  R2(obs=test.data$z.noisy,pred=nn.test.size5.pred))),\n",
    "       bty = \"n\")\n",
    "title(\"ANN with 5 nodes\\nin hidden layer\")\n",
    "plot.xyz.persp(x=data$x,y=data$y,z=nn.pred.size.50)\n",
    "legend(\"topright\",legend=c(sprintf(\"MSE: %.2f\",MSE(obs=test.data$z.noisy,pred=nn.test.size50.pred)),\n",
    "                           sprintf(\"R2: %.2f\",  R2(obs=test.data$z.noisy,pred=nn.test.size50.pred))),\n",
    "       bty = \"n\")\n",
    "title(\"ANN with 50 nodes\\nin hidden layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of different hidden layer sizes (from 1 to 200) is shown in more detail also on slide 27 of the last lecture:\n",
    "\n",
    "<img src=\"figures/ann_size.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions</b> \n",
    "\n",
    "- How could we counter potential over-fitting?\n",
    "- Besides increasing the hidden layer size, what could be an option to increase the predictive power of the model?\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation on ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have already heard short to a thousand times during this lecture, we use Cross-Validation to optimise hyperparameters of the model. This in principle also applies and works well for ANNs. However, since ANNs are very data-hungry and can training can easily become quite slow with a large amount of hidden units, Cross-Validation is usually quite expensive for ANNs. We will do it below for demonstration for the parameter `maxit`, but also note that the Lecture slides offer some alternative ways of avoiding overfitting while training an ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define number of CV-folds and obtain CV-folds:\n",
    "set.seed(123)\n",
    "cv.K <- 6\n",
    "cv.folds <- split(sample(1:N.train.data),\n",
    "                  rep(1:cv.K, length = N.train.data))\n",
    "\n",
    "## Define size of hidden layer:\n",
    "net.size <- 50\n",
    "\n",
    "## Define how many iterations should be tested:\n",
    "sel.maxit <- seq(100,600,by=100)\n",
    "print(paste(c(\"Number of iterations tested:\",sel.maxit,\"with\",cv.K,\"CV-folds each\"),\n",
    "            collapse=\" \"))\n",
    "\n",
    "## Prepare empty matrix for saving MSE results\n",
    "maxit.mse <- matrix(NA,ncol=cv.K,nrow=length(sel.maxit))\n",
    "colnames(maxit.mse) <- paste(\"K\",1:cv.K,sep=\".\")\n",
    "rownames(maxit.mse) <- paste(\"maxit\",sel.maxit,sep=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "ATTENTION: This cell will take a few minutes (~4min) to compute (time for ☕️😋)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop over the different number of maximum iterations:\n",
    "for(ii in seq_along(sel.maxit)) {\n",
    "    mi <- sel.maxit[ii]\n",
    "    cat(base::date(),\"i =\",ii,\" and maxit =\",mi,\"\\n\") ## print progress\n",
    "    \n",
    "    ## Now loop over the CV folds:\n",
    "    for(kk in seq_along(cv.folds)){\n",
    "        cat(\"\\t\",base::date(), \"CV K =\",kk,\"...\")\n",
    "        sel.fold <- cv.folds[[kk]]\n",
    "        \n",
    "        ## Fit model on training folds:\n",
    "        mi.fit <- nnet(z.noisy~x+y,\n",
    "                       data=train.data[-sel.fold,], ## omit fold from training\n",
    "                       size=net.size,maxit=mi,\n",
    "                       abstol = 0, ## disable tolerance based early stopping\n",
    "                       reltol = 0,\n",
    "                       linout=TRUE,trace=FALSE)\n",
    "        ## Predict on testing fold:\n",
    "        mi.test.pred <- predict(mi.fit,newdata=test.data[sel.fold,]) ## predict on \"sel.fold\" only\n",
    "        \n",
    "        ## GET MSE on testing fold:\n",
    "        maxit.mse[ii,kk] <- MSE(obs=test.data$z.noisy[sel.fold],pred=mi.test.pred)\n",
    "        cat(\" MSE =\",maxit.mse[ii,kk],\"\\n\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've had ample time for coffee, we can find the value for `maxit` with the lowest MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get mean MSE for different maximum iterations:\n",
    "mean.err <- rowMeans(maxit.mse)\n",
    "\n",
    "## Get standard deviation of MSE for different maximum iterations:\n",
    "sd.err <- apply(maxit.mse,1,sd)\n",
    "\n",
    "## Find smallest mean MSE\n",
    "sprintf(\"Lowest mean MSE: %.2f\", (min.err <- min(mean.err)))\n",
    "\n",
    "## number of iterations with the smallest error\n",
    "sprintf(\"Iteration maximum with lowest mean MSE: %i\", (opt_maxit <- sel.maxit[mean.err==min.err]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the results of the Cross-Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot CV results\n",
    "par(mar=c(4,4,1,1))\n",
    "yy.lim <- range(c(mean.err+sd.err,mean.err-sd.err))\n",
    "plot.new()\n",
    "plot.window(xlim=range(sel.maxit),ylim=yy.lim)\n",
    "segments(x0=sel.maxit,y0=mean.err+sd.err,y1=mean.err-sd.err,\n",
    "         col=\"darkgray\")\n",
    "points(sel.maxit,mean.err,pch=19,t=\"b\",col=\"black\")\n",
    "abline(h=min.err, col=\"red\"); points(opt_maxit, min.err, cex=2, col=\"red\")\n",
    "axis(1);axis(2);box()\n",
    "title(xlab=\"# training iterations\",ylab=\"MSE (10 fold cross validation)\")\n",
    "legend(\"topright\",legend=c(\"mean error\",\"sd error\"),\n",
    "       pch=c(19,NA),lty=c(NA,1),col=c(\"black\",\"darkgray\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions</b> \n",
    "\n",
    "- Would you say that `500` might be the optimal value for `maxit` in this setting? Can you think of arguments against it?\n",
    "- What are other methods for to control overfitting?\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted trees and ANN on PSL/precipitation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably remember well our PSL/precipitation dataset. As a last part in this tutorial, we will fit both a Boosted Trees algorithm and an ANN to the data and compare their predictive power. Remember the structure of the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | y | X_1 | X_2 | ... | X_p |\n",
    "| :- | :- | :- | :- | :- | :- |\n",
    "| | Precip_Zurich | PSL_lon$_1$lat$_1$ | PSL_lon$_2$lat$_1$ | .... | PSL_lon$_x$lat$_y$ |\n",
    "| December year$_1$ | 0.242 | 156 | 81.5 | ... | ... | \n",
    "| January year$_2$ | -1.115 | 63.5 | 78 | ... | ... | \n",
    "| February year$_2$ | 2.129 | 78 | 117 | ... | ... | \n",
    "| December year$_2$ | -1.574 | 128.81 | -207.7 | ... | ... | \n",
    "| January year$_3$ | -1.333 | 170.6 | 53.3 | ... | ... | \n",
    "| ... |... | ... | ... | ... | ... | \n",
    "| December year$_n$ | ... | ... | ... | ... | ... | \n",
    "| January year$_n$ | ... | ... | ... | ... | ... | \n",
    "| February year$_n$ | ... | ... | ... | ... | ... | \n",
    "\n",
    "<img src=\"figures/Precip_PSL.png\" width=300 height=450 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and divide into training and validation parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data from Exercise 2 Part 1:\n",
    "load(\"../Exercise3/data/Exercise3_Part1_Data.RData\")\n",
    "str(Precip_PSL_df[,1:3])\n",
    "\n",
    "## Rename predictor column names (otherwise the function call \"Precip_Zurich ~ .\" would not work):\n",
    "Precip_PSL_df_new <- Precip_PSL_df\n",
    "names(Precip_PSL_df_new)[2:ncol(Precip_PSL_df_new)] <- paste0(\"PSL\",1:(ncol(Precip_PSL_df_new)-1))\n",
    "\n",
    "## Specify training and validation indices:\n",
    "set.seed(1234)\n",
    "ind_train     <- sample(1:nrow(Precip_PSL_df), 2700)\n",
    "ind_test      <- which(!(1:nrow(Precip_PSL_df) %in% ind_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fit the boosted trees ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decide on function settings:\n",
    "distribution_set  <- \"gaussian\" ## For squared error loss (for regression problem)\n",
    "n_trees           <- 5          ## Number of trees\n",
    "interaction_depth <- 1          ## Tree size (number of splits)\n",
    "shrinkage_par     <- 1          ## \"Regularisation\"\n",
    "\n",
    "## Fit boosted trees to PSL/precip dataset:\n",
    "gbm.fit <- gbm(Precip_Zurich~., data=Precip_PSL_df_new[ind_train,],\n",
    "                  distribution=distribution_set,\n",
    "                  n.tree=n_trees,\n",
    "                  interaction.depth=interaction_depth,\n",
    "                  shrinkage=shrinkage_par,\n",
    "                  bag.fraction=1)\n",
    "\n",
    "\n",
    "## Make prediction with boosted tree model on test dataset:\n",
    "gbm.test.prediction <- predict(gbm.fit, newdata=Precip_PSL_df_new[ind_test,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit a neural network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set maximum number of iterations:\n",
    "nn.maxit <- 1000\n",
    "\n",
    "## Set size of hidden layer:\n",
    "n_nodes_h <- 10\n",
    "\n",
    "system.time(\n",
    "nnet.fit <- nnet(Precip_Zurich~.,data=Precip_PSL_df_new[ind_train,],\n",
    "                 size=n_nodes_h,\n",
    "                 maxit=nn.maxit,\n",
    "                 linout=TRUE, trace=F))\n",
    "\n",
    "## Make prediction with neural net model on test dataset:\n",
    "nnet.test.prediction <- predict(nnet.fit, newdata=Precip_PSL_df_new[ind_test,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions</b> \n",
    "\n",
    "- What happened here?! 😳 \n",
    "- What could we do to get rid of this error?\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to perform feature selection, i.e. only using the 10 closest PSL grid points as predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the indices of the 10 closest gridpoints to Zurich:\n",
    "n_closests_coord <- 10\n",
    "ZH_lon <- 8.5417\n",
    "ZH_lat <- 47.4\n",
    "\n",
    "## Read indices from names in data.frame and calculate distance:\n",
    "coord_vec        <- do.call(rbind,strsplit(names(Precip_PSL_df[,2:ncol(Precip_PSL_df)]),\"_\"))[,2:3]\n",
    "class(coord_vec) <- \"numeric\"\n",
    "dist_coord_ZH    <- dist(rbind(c(ZH_lon,ZH_lat), coord_vec))\n",
    "ind_closest      <- order(dist_coord_ZH[1:nrow(coord_vec)])[1:n_closests_coord]\n",
    "\n",
    "## Get subset of training data with gridpoints closest to Zurich:\n",
    "Precip_PSL_df_closeZH <- Precip_PSL_df[,c(1,ind_closest+1)]\n",
    "names(Precip_PSL_df_closeZH)[2:ncol(Precip_PSL_df_closeZH)] <- paste0(\"PSL\",1:(ncol(Precip_PSL_df_closeZH)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets fit this neural net to the predictor subset and decrease the hidden layer size slightely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set maximum number of iterations:\n",
    "nn.maxit <- 1000\n",
    "\n",
    "## Set size of hidden layer:\n",
    "n_nodes_h <- 5\n",
    "\n",
    "## Fit model to subset of predictors:\n",
    "nnet.fit <- nnet(Precip_Zurich~.,data=Precip_PSL_df_closeZH[ind_train,],\n",
    "                 size=n_nodes_h,\n",
    "                 maxit=nn.maxit,\n",
    "                 linout=TRUE, trace=F)\n",
    "\n",
    "## Make prediction with neural net model on test dataset:\n",
    "nnet.test.prediction <- predict(nnet.fit, newdata=Precip_PSL_df_closeZH[ind_test,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot network and weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ann(n_input=n_closests_coord,n_hidden=n_nodes_h,weights=nnet.fit$wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the performance of the Boosted Trees and the ANN on predicting precipitation in Zürich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare prediction with validation (test) data:\n",
    "plot_range <- range(Precip_PSL_df$Precip_Zurich[ind_test],nnet.test.prediction)\n",
    "plot(Precip_PSL_df$Precip_Zurich[ind_test],nnet.test.prediction,col=\"salmon\",\n",
    "     xlab=\"True precip values [mm/d]\",ylab=\"Predicted precip values [mm/d]\",\n",
    "     xlim=plot_range,ylim=plot_range, pch=1)\n",
    "points(Precip_PSL_df$Precip_Zurich[ind_test],gbm.test.prediction,col=\"navyblue\", pch=2)\n",
    "legend(\"topleft\",\n",
    "       legend=c(sprintf(\"ANN (%.2f / %.2f)\",\n",
    "                        MSE(Precip_PSL_df$Precip_Zurich[ind_test],nnet.test.prediction),\n",
    "                        R2(Precip_PSL_df$Precip_Zurich[ind_test],nnet.test.prediction)),\n",
    "                sprintf(\"Boosted trees (%.2f / %.2f)\",\n",
    "                        MSE(Precip_PSL_df$Precip_Zurich[ind_test],gbm.test.prediction),\n",
    "                        R2(Precip_PSL_df$Precip_Zurich[ind_test],gbm.test.prediction))),\n",
    "      pch=c(1,2,5),col=c(\"salmon\",\"navyblue\",\"darkgreen\"), title=\"Model (MSE / R2)\")\n",
    "abline(0,1,col=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions</b> \n",
    "    \n",
    "- Explaining the banded predictions of the boosted tree is easy, but can you also explain the banded predictions of the ANN?\n",
    "- Try to (manually) optimise the parameters of the boosted tree.\n",
    "- Can you find an network architecture (number of predictor gridpoints, hidden layer size) which is able to beat the boosted tree prediction?\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Bonus] Manually predict ANN using Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand how an ANN gets from input datapoints to the prediction, we can manually predict the outcome of the ANN. For that, we only need to know the network structure, the weights, and the activation function and perform a bit of Matrix Multiplication. We use the network structure, the weights and the activation function from the ANN above with 5 hidden units.\n",
    "\n",
    "The activation function in ```nnet``` is written in ```C``` (according to https://stats.stackexchange.com/questions/78252/whats-the-activation-function-used-in-the-nodes-of-hidden-layer-from-nnet-libra), defined as:\n",
    "\n",
    "```C\n",
    "static double\n",
    "sigmoid(double sum)\n",
    "{\n",
    "    if (sum < -15.0)\n",
    "    return (0.0);\n",
    "    else if (sum > 15.0)\n",
    "    return (1.0);\n",
    "    else\n",
    "    return (1.0 / (1.0 + exp(-sum)));\n",
    "}\n",
    "```\n",
    "so we implement it in ```R``` the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define sigmoid function manually:\n",
    "sigmoid_activation <- function(sum) {\n",
    "    if(sum< -15) return(0.0) \n",
    "    else if(sum> 15) return(1.0)\n",
    "    else return(1./(1+exp(-sum)))\n",
    "}\n",
    "sigmoid_activation_vec <- Vectorize(sigmoid_activation)\n",
    "\n",
    "## Plot activation function:\n",
    "x_sigmoid <- seq(-16,16,0.01)\n",
    "plot(x_sigmoid, sigmoid_activation_vec(x_sigmoid), type=\"l\",xlab=\"x\",\n",
    "     ylab=bquote(\"sigmoid(x) =\"~1/(1+e^-x)), col=\"deepskyblue3\", lwd=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly select a number of points `n_pred`from our x-y-domain (-10 to plus 10 in both directions) and save them in a small dataframe `input_df` with the format (rows, columns) = (datapoints, features), where features are the input variables (`x` and `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For how many random points should a prediction be made:\n",
    "n_pred <- 1\n",
    "\n",
    "## Make automatic and manual prediction:\n",
    "x_pred <- runif(n_pred,-10,10); y_pred <- runif(n_pred,-10,10)\n",
    "sprintf(\"Make prediction for points\")\n",
    "(input_df <- data.frame(x=x_pred,y=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our already trained ANN from above and predict the points in `input_df` to compare with our manual prediction at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make prediction using 'predict' function:\n",
    "sprintf(\"Prediction: %.2f\", predict(nn.fit.size5,newdata=input_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the trained ANN, we extract the weights (coefficients) of all input, hidden and output nodes plus all bias coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get fitted coefficients:\n",
    "all_coef <- coef(nn.fit.size5)\n",
    "weights_i_h <- all_coef[grep(\"i\", names(all_coef))]\n",
    "weights_h_o <- all_coef[grep(\"h.->o\", names(all_coef))]\n",
    "bias_h      <- all_coef[grep(\"b->h\", names(all_coef))]\n",
    "bias_o      <- all_coef[grep(\"b->o\", names(all_coef))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save all lists extracted above as matrices and are ready to unpack our Matrix Multiplication skills from our math courses in the first semester again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From weight matrices and bias vectors:\n",
    "sprintf(\"Input matrix (1x2):\")\n",
    "(input_matrix <- as.matrix(input_df))\n",
    "\n",
    "sprintf(\"Weight matrix (2x5) inputs -> hidden layer:\")\n",
    "(weight_matrix_i_h <- matrix(weights_i_h,ncol=5))\n",
    "\n",
    "sprintf(\"Weight matrix (5x1) hidden layer -> output:\")\n",
    "(weight_matrix_h_o <- matrix(weights_h_o,nrow=5,byrow=T))\n",
    "\n",
    "sprintf(\"Bias vector (1x5) hidden layer:\")\n",
    "(bias_matrix_h <- matrix(bias_h,ncol=5))\n",
    "\n",
    "sprintf(\"Bias vector (1x1) output:\")\n",
    "(bias_matrix_o <- matrix(bias_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of arriving at a prediction, given the input $x_i$, weights $w_{ij}$ $w_{jk}$, network structure and activation function $\\sigma$ is described on slide 25 of the previous lecture:\n",
    "\n",
    "<img src=\"figures/forward_propagation.png\" width=300 />\n",
    "\n",
    "This formula can equivalently be expressed in matrix notation:\n",
    "\n",
    "$y = \\sigma(XW_{inner} + B_{hidden})W_{outer} + B_{output}$\n",
    "\n",
    "where $X$ are the inputs, $W_{inner}$ are the weights $w_{ij}$ connecting input layer and hidden layer, $W_{outer}$ are the weights $w_{jk}$ connection hidden layer and output layer and $B$ are the bias vectors.\n",
    "\n",
    "We calculate the resulting $y$ vector by applying matrix multiplication from left (input nodes) to right (output nodes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprintf(\"Multiplication of input matrix and weights(i->h):\")\n",
    "(mat_iw <- input_matrix %*% weight_matrix_i_h)\n",
    "\n",
    "sprintf(\"Adding the biases of the hidden layer nodes:\")\n",
    "(mat_iwb <- mat_iw + matrix(rep(bias_matrix_h,nrow(mat_iw)),\n",
    "                            nrow=nrow(mat_iw),byrow=T))\n",
    "\n",
    "sprintf(\"Apply sigmoid function (this is where the magic happens):\")\n",
    "(mat_siwb <- matrix(sigmoid_activation_vec(mat_iwb),nrow=nrow(mat_iwb)))\n",
    "\n",
    "sprintf(\"Multiply weights(h->o):\")\n",
    "(mat_wsiwb <- mat_siwb %*% weight_matrix_h_o)\n",
    "\n",
    "sprintf(\"Adding the bias of the output node to obtain the final prediction:\")\n",
    "(mat_wsiwbb <- mat_wsiwb + rep(bias_matrix_o,nrow(mat_wsiwb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions</b> \n",
    "\n",
    "- Do the two predictions agree?\n",
    "- Try to follow the prediction steps in the cell above, does it make sense to you?\n",
    "- Increase the number of prediction points by setting ```n_pred```to a higher integer.\n",
    "\n",
    "</div> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R (iacr_2020_test)",
   "language": "R",
   "name": "iacr_2020_test"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
